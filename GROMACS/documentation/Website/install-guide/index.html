
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Installation guide &#8212; GROMACS 2018.3 documentation</title>
    
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2018.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="User guide" href="../user-guide/index.html" />
    <link rel="prev" title="Release notes for older GROMACS versions" href="../release-notes/older/index.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../user-guide/index.html" title="User guide"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../release-notes/older/index.html" title="Release notes for older GROMACS versions"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">GROMACS 2018.3</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="installation-guide">
<span id="install-guide"></span><h1>Installation guide<a class="headerlink" href="#installation-guide" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-to-building-gromacs">
<h2>Introduction to building GROMACS<a class="headerlink" href="#introduction-to-building-gromacs" title="Permalink to this headline">¶</a></h2>
<p>These instructions pertain to building GROMACS
2018.3. You might also want to check the <a class="reference external" href="http://www.gromacs.org/Documentation/Installation_Instructions">up-to-date installation instructions</a>.</p>
<div class="section" id="quick-and-dirty-installation">
<h3>Quick and dirty installation<a class="headerlink" href="#quick-and-dirty-installation" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Get the latest version of your C and C++ compilers.</li>
<li>Check that you have CMake version 3.4.3 or later.</li>
<li>Get and unpack the latest version of the GROMACS tarball.</li>
<li>Make a separate build directory and change to it.</li>
<li>Run <code class="docutils literal"><span class="pre">cmake</span></code> with the path to the source as an argument</li>
<li>Run <code class="docutils literal"><span class="pre">make</span></code>, <code class="docutils literal"><span class="pre">make</span> <span class="pre">check</span></code>, and <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code></li>
<li>Source <code class="docutils literal"><span class="pre">GMXRC</span></code> to get access to GROMACS</li>
</ol>
<p>Or, as a sequence of commands to execute:</p>
<pre class="literal-block">
tar xfz gromacs-2018.3.tar.gz
cd gromacs-2018.3
mkdir build
cd build
cmake .. -DGMX_BUILD_OWN_FFTW=ON -DREGRESSIONTEST_DOWNLOAD=ON
make
make check
sudo make install
source /usr/local/gromacs/bin/GMXRC
</pre>
<p>This will download and build first the prerequisite FFT library
followed by GROMACS. If you already have FFTW installed, you can
remove that argument to <code class="docutils literal"><span class="pre">cmake</span></code>. Overall, this build of GROMACS
will be correct and reasonably fast on the machine upon which
<code class="docutils literal"><span class="pre">cmake</span></code> ran. On another machine, it may not run, or may not run
fast. If you want to get the maximum value for your hardware with
GROMACS, you will have to read further. Sadly, the interactions of
hardware, libraries, and compilers are only going to continue to get
more complex.</p>
</div>
<div class="section" id="quick-and-dirty-cluster-installation">
<h3>Quick and dirty cluster installation<a class="headerlink" href="#quick-and-dirty-cluster-installation" title="Permalink to this headline">¶</a></h3>
<p>On a cluster where users are expected to be running across multiple
nodes using MPI, make one installation similar to the above, and
another using an MPI wrapper compiler and which is <a class="reference internal" href="#building-only-mdrun">building only
mdrun</a>, because that is the only component of GROMACS that uses
MPI. The latter will install a single simulation engine binary,
i.e. <code class="docutils literal"><span class="pre">mdrun_mpi</span></code> when the default suffix is used. Hence it is safe
and common practice to install this into the same location where
the non-MPI build is installed.</p>
</div>
<div class="section" id="typical-installation">
<h3>Typical installation<a class="headerlink" href="#typical-installation" title="Permalink to this headline">¶</a></h3>
<p>As above, and with further details below, but you should consider
using the following <a class="reference internal" href="#cmake-options">CMake options</a> with the
appropriate value instead of <code class="docutils literal"><span class="pre">xxx</span></code> :</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">-DCMAKE_C_COMPILER=xxx</span></code> equal to the name of the C99 <a class="reference internal" href="#compiler">Compiler</a> you wish to use (or the environment variable <code class="docutils literal"><span class="pre">CC</span></code>)</li>
<li><code class="docutils literal"><span class="pre">-DCMAKE_CXX_COMPILER=xxx</span></code> equal to the name of the C++98 <a class="reference internal" href="#compiler">compiler</a> you wish to use (or the environment variable <code class="docutils literal"><span class="pre">CXX</span></code>)</li>
<li><code class="docutils literal"><span class="pre">-DGMX_MPI=on</span></code> to build using <a class="reference internal" href="#id1">MPI support</a> (generally good to combine with <a class="reference internal" href="#building-only-mdrun">building only mdrun</a>)</li>
<li><code class="docutils literal"><span class="pre">-DGMX_GPU=on</span></code> to build using nvcc to run using NVIDIA <a class="reference internal" href="#cuda-gpu-acceleration">CUDA GPU acceleration</a> or an <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> GPU</li>
<li><code class="docutils literal"><span class="pre">-DGMX_USE_OPENCL=on</span></code> to build with <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> support enabled. <code class="docutils literal"><span class="pre">GMX_GPU</span></code> must also be set.</li>
<li><code class="docutils literal"><span class="pre">-DGMX_SIMD=xxx</span></code> to specify the level of <a class="reference internal" href="#simd-support">SIMD support</a> of the node on which GROMACS will run</li>
<li><code class="docutils literal"><span class="pre">-DGMX_BUILD_MDRUN_ONLY=on</span></code> for <a class="reference internal" href="#building-only-mdrun">building only mdrun</a>, e.g. for compute cluster back-end nodes</li>
<li><code class="docutils literal"><span class="pre">-DGMX_DOUBLE=on</span></code> to build GROMACS in double precision (slower, and not normally useful)</li>
<li><code class="docutils literal"><span class="pre">-DCMAKE_PREFIX_PATH=xxx</span></code> to add a non-standard location for CMake to <a class="reference internal" href="#search-for-libraries-headers-or-programs">search for libraries, headers or programs</a></li>
<li><code class="docutils literal"><span class="pre">-DCMAKE_INSTALL_PREFIX=xxx</span></code> to install GROMACS to a <a class="reference internal" href="#non-standard-location">non-standard location</a> (default <code class="docutils literal"><span class="pre">/usr/local/gromacs</span></code>)</li>
<li><code class="docutils literal"><span class="pre">-DBUILD_SHARED_LIBS=off</span></code> to turn off the building of shared libraries to help with <a class="reference internal" href="#static-linking">static linking</a></li>
<li><code class="docutils literal"><span class="pre">-DGMX_FFT_LIBRARY=xxx</span></code> to select whether to use <code class="docutils literal"><span class="pre">fftw</span></code>, <code class="docutils literal"><span class="pre">mkl</span></code> or <code class="docutils literal"><span class="pre">fftpack</span></code> libraries for <a class="reference internal" href="#fft-support">FFT support</a></li>
<li><code class="docutils literal"><span class="pre">-DCMAKE_BUILD_TYPE=Debug</span></code> to build GROMACS in debug mode</li>
</ul>
</div>
<div class="section" id="building-older-versions">
<h3>Building older versions<a class="headerlink" href="#building-older-versions" title="Permalink to this headline">¶</a></h3>
<p>Installation instructions for old GROMACS versions can be found at
the GROMACS <a class="reference external" href="http://manual.gromacs.org/documentation">documentation page</a>.</p>
</div>
</div>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<div class="section" id="platform">
<h3>Platform<a class="headerlink" href="#platform" title="Permalink to this headline">¶</a></h3>
<p>GROMACS can be compiled for many operating systems and
architectures.  These include any distribution of Linux, Mac OS X or
Windows, and architectures including x86, AMD64/x86-64, several
PowerPC including POWER8, ARM v7, ARM v8, and SPARC VIII.</p>
</div>
<div class="section" id="compiler">
<h3>Compiler<a class="headerlink" href="#compiler" title="Permalink to this headline">¶</a></h3>
<p>GROMACS can be compiled on any platform with ANSI C99 and C++11
compilers, and their respective standard C/C++ libraries. Good
performance on an OS and architecture requires choosing a good
compiler. We recommend gcc, because it is free, widely available and
frequently provides the best performance.</p>
<p>You should strive to use the most recent version of your
compiler. Since we require full C++11 support the minimum supported
compiler versions are</p>
<ul class="simple">
<li>GNU (gcc) 4.8.1</li>
<li>Intel (icc) 15.0</li>
<li>LLVM (clang) 3.3</li>
<li>Microsoft (MSVC) 2015</li>
</ul>
<p>Other compilers may work (Cray, Pathscale, older clang) but do
not offer competitive performance. We recommend against PGI because
the performance with C++ is very bad.</p>
<p>The xlc compiler is not supported and has not been tested on POWER
architectures for GROMACS-2018.3. We recommend to use the gcc
compiler instead, as it is being extensively tested.</p>
<p>You may also need the most recent version of other compiler toolchain
components beside the compiler itself (e.g. assembler or linker);
these are often shipped by your OS distribution’s binutils package.</p>
<p>C++11 support requires adequate support in both the compiler and the
C++ library. The gcc and MSVC compilers include their own standard
libraries and require no further configuration. For configuration of
other compilers, read on.</p>
<p>On Linux, both the Intel and clang compiler use the libstdc++ which
comes with gcc as the default C++ library. For GROMACS, we require
the compiler to support libstc++ version 4.8.1 or higher. To select a
particular libstdc++ library, use:</p>
<ul class="simple">
<li>For Intel: <code class="docutils literal"><span class="pre">-DGMX_STDLIB_CXX_FLAGS=-gcc-name=/path/to/gcc/binary</span></code>
or make sure that the correct gcc version is first in path (e.g. by
loading the gcc module). It can also be useful to add
<code class="docutils literal"><span class="pre">-DCMAKE_CXX_LINK_FLAGS=&quot;-Wl,-rpath,/path/to/gcc/lib64</span>
<span class="pre">-L/path/to/gcc/lib64&quot;</span></code> to ensure linking works correctly.</li>
<li>For clang:
<code class="docutils literal"><span class="pre">-DCMAKE_CXX_FLAGS=--gcc-toolchain=/path/to/gcc/folder</span></code>. This
folder should contain <code class="docutils literal"><span class="pre">include/c++</span></code>.</li>
</ul>
<p>On Windows with the Intel compiler, the MSVC standard library is used,
and at least MSVC 2015 is required. Load the enviroment variables with
vcvarsall.bat.</p>
<p>To build with any compiler and clang’s libcxx standard library, use
<code class="docutils literal"><span class="pre">-DGMX_STDLIB_CXX_FLAGS=-stdlib=libc++</span>
<span class="pre">-DGMX_STDLIB_LIBRARIES='-lc++abi</span> <span class="pre">-lc++'</span></code>.</p>
<p>If you are running on Mac OS X, the best option is the Intel
compiler. Both clang and gcc will work, but they produce lower
performance and each have some shortcomings. clang 3.8 now offers
support for OpenMP, and so may provide decent performance.</p>
<p>For all non-x86 platforms, your best option is typically to use gcc or
the vendor’s default or recommended compiler, and check for
specialized information below.</p>
<p>For updated versions of gcc to add to your Linux OS, see</p>
<ul class="simple">
<li>Ubuntu: <a class="reference external" href="https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test">Ubuntu toolchain ppa page</a></li>
<li>RHEL/CentOS: <a class="reference external" href="https://fedoraproject.org/wiki/EPEL">EPEL page</a> or the RedHat Developer Toolset</li>
</ul>
</div>
<div class="section" id="compiling-with-parallelization-options">
<h3>Compiling with parallelization options<a class="headerlink" href="#compiling-with-parallelization-options" title="Permalink to this headline">¶</a></h3>
<p>For maximum performance you will need to examine how you will use
GROMACS and what hardware you plan to run on. Often <a class="reference external" href="http://en.wikipedia.org/wiki/OpenMP">OpenMP</a>
parallelism is an advantage for GROMACS, but support for this is
generally built into your compiler and detected automatically.</p>
<div class="section" id="gpu-support">
<h4>GPU support<a class="headerlink" href="#gpu-support" title="Permalink to this headline">¶</a></h4>
<p>GROMACS has excellent support for NVIDIA GPUs supported via CUDA.
On Linux, NVIDIA <a class="reference external" href="http://www.nvidia.com/object/cuda_home_new.html">CUDA</a> toolkit with minimum version 6.5
is required, and the latest
version is strongly encouraged. Using Intel or Microsoft MSVC compilers
requires version 7.0 and 8.0, respectively. NVIDIA GPUs with at
least NVIDIA compute capability 2.0 are
required. You are strongly recommended to
get the latest CUDA version and driver that supports your hardware, but
beware of possible performance regressions in newer CUDA versions on
older hardware. Note that compute capability 2.0 (Fermi)
devices are no longer supported from CUDA 9.0 and later.
While some CUDA compilers (nvcc) might not
officially support recent versions of gcc as the back-end compiler, we
still recommend that you at least use a gcc version recent enough to
get the best SIMD support for your CPU, since GROMACS always runs some
code on the CPU. It is most reliable to use the same C++ compiler
version for GROMACS code as used as the host compiler for nvcc.</p>
<p>To make it possible to use other accelerators, GROMACS also includes
<a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> support. The minimum OpenCL version required is
1.1. The current OpenCL implementation is recommended for
use with GCN-based AMD GPUs, on Linux we recommend the ROCm runtime.
It is also supported with NVIDIA GPUs, but using
the latest NVIDIA driver (which includes the NVIDIA OpenCL runtime) is
recommended. Also note that there are performance limitations (inherent
to the NVIDIA OpenCL runtime).
It is not possible to configure both CUDA and OpenCL
support in the same version of GROMACS.</p>
</div>
<div class="section" id="mpi-support">
<span id="id1"></span><h4>MPI support<a class="headerlink" href="#mpi-support" title="Permalink to this headline">¶</a></h4>
<p>GROMACS can run in parallel on multiple cores of a single
workstation using its built-in thread-MPI. No user action is required
in order to enable this.</p>
<p>If you wish to run in parallel on multiple machines across a network,
you will need to have</p>
<ul class="simple">
<li>an MPI library installed that supports the MPI 1.3
standard, and</li>
<li>wrapper compilers that will compile code using that library.</li>
</ul>
<p>The GROMACS team recommends <a class="reference external" href="http://www.open-mpi.org">OpenMPI</a> version
1.6 (or higher), <a class="reference external" href="http://www.mpich.org">MPICH</a> version 1.4.1 (or
higher), or your hardware vendor’s MPI installation. The most recent
version of either of these is likely to be the best. More specialized
networks might depend on accelerations only available in the vendor’s
library. <a class="reference external" href="http://www.lam-mpi.org">LAM-MPI</a> might work, but since it has
been deprecated for years, it is not supported.</p>
</div>
</div>
<div class="section" id="cmake">
<h3>CMake<a class="headerlink" href="#cmake" title="Permalink to this headline">¶</a></h3>
<p>GROMACS builds with the CMake build system, requiring at least
version 3.4.3. You can check whether
CMake is installed, and what version it is, with <code class="docutils literal"><span class="pre">cmake</span>
<span class="pre">--version</span></code>. If you need to install CMake, then first check whether
your platform’s package management system provides a suitable version,
or visit the <a class="reference external" href="http://www.cmake.org/install/">CMake installation page</a> for pre-compiled binaries,
source code and installation instructions. The GROMACS team
recommends you install the most recent version of CMake you can.</p>
</div>
<div class="section" id="fast-fourier-transform-library">
<span id="fft-support"></span><h3>Fast Fourier Transform library<a class="headerlink" href="#fast-fourier-transform-library" title="Permalink to this headline">¶</a></h3>
<p>Many simulations in GROMACS make extensive use of fast Fourier
transforms, and a software library to perform these is always
required. We recommend <a class="reference external" href="http://www.fftw.org">FFTW</a> (version 3 or higher only) or Intel
<a class="reference external" href="https://software.intel.com/en-us/intel-mkl">MKL</a>. The choice of library can be set with <code class="docutils literal"><span class="pre">cmake</span>
<span class="pre">-DGMX_FFT_LIBRARY=&lt;name&gt;</span></code>, where <code class="docutils literal"><span class="pre">&lt;name&gt;</span></code> is one of <code class="docutils literal"><span class="pre">fftw</span></code>,
<code class="docutils literal"><span class="pre">mkl</span></code>, or <code class="docutils literal"><span class="pre">fftpack</span></code>. FFTPACK is bundled with GROMACS as a
fallback, and is acceptable if simulation performance is not a
priority. When choosing MKL, GROMACS will also use MKL for BLAS and
LAPACK (see <a class="reference internal" href="#linear-algebra-libraries">linear algebra libraries</a>). Generally, there is no
advantage in using MKL with GROMACS, and FFTW is often faster.
With PME GPU offload support using CUDA, a GPU-based FFT library
is required. The CUDA-based GPU FFT library cuFFT is part of the
CUDA toolkit (required for all CUDA builds) and therefore no additional
software component is needed when building with CUDA GPU acceleration.</p>
<div class="section" id="using-fftw">
<h4>Using FFTW<a class="headerlink" href="#using-fftw" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://www.fftw.org">FFTW</a> is likely to be available for your platform via its package
management system, but there can be compatibility and significant
performance issues associated with these packages. In particular,
GROMACS simulations are normally run in “mixed” floating-point
precision, which is suited for the use of single precision in
FFTW. The default FFTW package is normally in double
precision, and good compiler options to use for FFTW when linked to
GROMACS may not have been used. Accordingly, the GROMACS team
recommends either</p>
<ul class="simple">
<li>that you permit the GROMACS installation to download and
build FFTW from source automatically for you (use
<code class="docutils literal"><span class="pre">cmake</span> <span class="pre">-DGMX_BUILD_OWN_FFTW=ON</span></code>), or</li>
<li>that you build FFTW from the source code.</li>
</ul>
<p>If you build FFTW from source yourself, get the most recent version
and follow the <a class="reference external" href="http://www.fftw.org/doc/Installation-and-Customization.html#Installation-and-Customization">FFTW installation guide</a>. Choose the precision for
FFTW (i.e. single/float vs. double) to match whether you will later
use mixed or double precision for GROMACS. There is no need to
compile FFTW with threading or MPI support, but it does no harm. On
x86 hardware, compile with <em>both</em> <code class="docutils literal"><span class="pre">--enable-sse2</span></code> and
<code class="docutils literal"><span class="pre">--enable-avx</span></code> for FFTW-3.3.4 and earlier. From FFTW-3.3.5, you
should also add <code class="docutils literal"><span class="pre">--enable-avx2</span></code> also. On Intel processors supporting
512-wide AVX, including KNL, add <code class="docutils literal"><span class="pre">--enable-avx512</span></code> also.
FFTW will create a fat library with codelets for all different instruction sets,
and pick the fastest supported one at runtime.
On ARM architectures with NEON SIMD support and IBM Power8 and later, you
definitely want version 3.3.5 or later,
and to compile it with <code class="docutils literal"><span class="pre">--enable-neon</span></code> and <code class="docutils literal"><span class="pre">--enable-vsx</span></code>, respectively, for
SIMD support. If you are using a Cray, there is a special modified
(commercial) version of FFTs using the FFTW interface which can be
slightly faster.</p>
</div>
<div class="section" id="using-mkl">
<h4>Using MKL<a class="headerlink" href="#using-mkl" title="Permalink to this headline">¶</a></h4>
<p>Use MKL bundled with Intel compilers by setting up the compiler
environment, e.g., through <code class="docutils literal"><span class="pre">source</span> <span class="pre">/path/to/compilervars.sh</span> <span class="pre">intel64</span></code>
or similar before running CMake including setting
<code class="docutils literal"><span class="pre">-DGMX_FFT_LIBRARY=mkl</span></code>.</p>
<p>If you need to customize this further, use</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -DGMX_FFT_LIBRARY<span class="o">=</span>mkl <span class="se">\</span>
      -DMKL_LIBRARIES<span class="o">=</span><span class="s2">&quot;/full/path/to/libone.so;/full/path/to/libtwo.so&quot;</span> <span class="se">\</span>
      -DMKL_INCLUDE_DIR<span class="o">=</span><span class="s2">&quot;/full/path/to/mkl/include&quot;</span>
</pre></div>
</div>
<p>The full list and order(!) of libraries you require are found in Intel’s MKL documentation for your system.</p>
</div>
</div>
<div class="section" id="other-optional-build-components">
<h3>Other optional build components<a class="headerlink" href="#other-optional-build-components" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Run-time detection of hardware capabilities can be improved by
linking with hwloc, which is automatically enabled if detected.</li>
<li>Hardware-optimized BLAS and LAPACK libraries are useful
for a few of the GROMACS utilities focused on normal modes and
matrix manipulation, but they do not provide any benefits for normal
simulations. Configuring these is discussed at
<a class="reference internal" href="#linear-algebra-libraries">linear algebra libraries</a>.</li>
<li>The built-in GROMACS trajectory viewer <code class="docutils literal"><span class="pre">gmx</span> <span class="pre">view</span></code> requires X11 and
Motif/Lesstif libraries and header files. You may prefer to use
third-party software for visualization, such as <a class="reference external" href="http://www.ks.uiuc.edu/Research/vmd/">VMD</a> or <a class="reference external" href="http://www.pymol.org">PyMol</a>.</li>
<li>An external TNG library for trajectory-file handling can be used
by setting <code class="docutils literal"><span class="pre">-DGMX_EXTERNAL_TNG=yes</span></code>, but TNG
1.7.10 is bundled in the GROMACS
source already.</li>
<li>An external lmfit library for Levenberg-Marquardt curve fitting
can be used by setting <code class="docutils literal"><span class="pre">-DGMX_EXTERNAL_LMFIT=yes</span></code>, but lmfit
6.1 is bundled in the GROMACS
source already.</li>
<li>zlib is used by TNG for compressing some kinds of trajectory data</li>
<li>Building the GROMACS documentation is optional, and requires
ImageMagick, pdflatex, bibtex, doxygen, python 2.7, sphinx
1.4.1, and pygments.</li>
<li>The GROMACS utility programs often write data files in formats
suitable for the Grace plotting tool, but it is straightforward to
use these files in other plotting programs, too.</li>
</ul>
</div>
</div>
<div class="section" id="doing-a-build-of-gromacs">
<h2>Doing a build of GROMACS<a class="headerlink" href="#doing-a-build-of-gromacs" title="Permalink to this headline">¶</a></h2>
<p>This section will cover a general build of GROMACS with <a class="reference internal" href="#cmake">CMake</a>, but it
is not an exhaustive discussion of how to use CMake. There are many
resources available on the web, which we suggest you search for when
you encounter problems not covered here. The material below applies
specifically to builds on Unix-like systems, including Linux, and Mac
OS X. For other platforms, see the specialist instructions below.</p>
<div class="section" id="configuring-with-cmake">
<span id="configure-cmake"></span><h3>Configuring with CMake<a class="headerlink" href="#configuring-with-cmake" title="Permalink to this headline">¶</a></h3>
<p>CMake will run many tests on your system and do its best to work out
how to build GROMACS for you. If your build machine is the same as
your target machine, then you can be sure that the defaults and
detection will be pretty good. However, if you want to control aspects
of the build, or you are compiling on a cluster head node for back-end
nodes with a different architecture, there are a few things you
should consider specifying.</p>
<p>The best way to use CMake to configure GROMACS is to do an
“out-of-source” build, by making another directory from which you will
run CMake. This can be outside the source directory, or a subdirectory
of it. It also means you can never corrupt your source code by trying
to build it! So, the only required argument on the CMake command line
is the name of the directory containing the <code class="docutils literal"><span class="pre">CMakeLists.txt</span></code> file of
the code you want to build. For example, download the source tarball
and use</p>
<pre class="literal-block">
tar xfz gromacs-2018.3.tgz
cd gromacs-2018.3
mkdir build-gromacs
cd build-gromacs
cmake ..
</pre>
<p>You will see <code class="docutils literal"><span class="pre">cmake</span></code> report a sequence of results of tests and
detections done by the GROMACS build system. These are written to the
<code class="docutils literal"><span class="pre">cmake</span></code> cache, kept in <code class="docutils literal"><span class="pre">CMakeCache.txt</span></code>. You can edit this file by
hand, but this is not recommended because you could make a mistake.
You should not attempt to move or copy this file to do another build,
because file paths are hard-coded within it. If you mess things up,
just delete this file and start again with <code class="docutils literal"><span class="pre">cmake</span></code>.</p>
<p>If there is a serious problem detected at this stage, then you will see
a fatal error and some suggestions for how to overcome it. If you are
not sure how to deal with that, please start by searching on the web
(most computer problems already have known solutions!) and then
consult the gmx-users mailing list. There are also informational
warnings that you might like to take on board or not. Piping the
output of <code class="docutils literal"><span class="pre">cmake</span></code> through <code class="docutils literal"><span class="pre">less</span></code> or <code class="docutils literal"><span class="pre">tee</span></code> can be
useful, too.</p>
<p>Once <code class="docutils literal"><span class="pre">cmake</span></code> returns, you can see all the settings that were chosen
and information about them by using e.g. the curses interface</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>ccmake ..
</pre></div>
</div>
<p>You can actually use <code class="docutils literal"><span class="pre">ccmake</span></code> (available on most Unix platforms)
directly in the first step, but then
most of the status messages will merely blink in the lower part
of the terminal rather than be written to standard output. Most platforms
including Linux, Windows, and Mac OS X even have native graphical user interfaces for
<code class="docutils literal"><span class="pre">cmake</span></code>, and it can create project files for almost any build environment
you want (including Visual Studio or Xcode).
Check out <a class="reference external" href="http://www.cmake.org/runningcmake/">running CMake</a> for
general advice on what you are seeing and how to navigate and change
things. The settings you might normally want to change are already
presented. You may make changes, then re-configure (using <code class="docutils literal"><span class="pre">c</span></code>), so that it
gets a chance to make changes that depend on yours and perform more
checking. It may take several configuration passes to reach the desired
configuration, in particular if you need to resolve errors.</p>
<p>When you have reached the desired configuration with <code class="docutils literal"><span class="pre">ccmake</span></code>, the
build system can be generated by pressing <code class="docutils literal"><span class="pre">g</span></code>.  This requires that the previous
configuration pass did not reveal any additional settings (if it did, you need
to configure once more with <code class="docutils literal"><span class="pre">c</span></code>).  With <code class="docutils literal"><span class="pre">cmake</span></code>, the build system is generated
after each pass that does not produce errors.</p>
<p>You cannot attempt to change compilers after the initial run of
<code class="docutils literal"><span class="pre">cmake</span></code>. If you need to change, clean up, and start again.</p>
<div class="section" id="where-to-install-gromacs">
<span id="non-standard-location"></span><h4>Where to install GROMACS<a class="headerlink" href="#where-to-install-gromacs" title="Permalink to this headline">¶</a></h4>
<p>GROMACS is installed in the directory to which
<code class="docutils literal"><span class="pre">CMAKE_INSTALL_PREFIX</span></code> points. It may not be the source directory or
the build directory.  You require write permissions to this
directory. Thus, without super-user privileges,
<code class="docutils literal"><span class="pre">CMAKE_INSTALL_PREFIX</span></code> will have to be within your home directory.
Even if you do have super-user privileges, you should use them only
for the installation phase, and never for configuring, building, or
running GROMACS!</p>
</div>
<div class="section" id="using-cmake-command-line-options">
<span id="cmake-options"></span><h4>Using CMake command-line options<a class="headerlink" href="#using-cmake-command-line-options" title="Permalink to this headline">¶</a></h4>
<p>Once you become comfortable with setting and changing options, you may
know in advance how you will configure GROMACS. If so, you can speed
things up by invoking <code class="docutils literal"><span class="pre">cmake</span></code> and passing the various options at once
on the command line. This can be done by setting cache variable at the
cmake invocation using <code class="docutils literal"><span class="pre">-DOPTION=VALUE</span></code>. Note that some
environment variables are also taken into account, in particular
variables like <code class="docutils literal"><span class="pre">CC</span></code> and <code class="docutils literal"><span class="pre">CXX</span></code>.</p>
<p>For example, the following command line</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DGMX_GPU<span class="o">=</span>ON -DGMX_MPI<span class="o">=</span>ON -DCMAKE_INSTALL_PREFIX<span class="o">=</span>/home/marydoe/programs
</pre></div>
</div>
<p>can be used to build with CUDA GPUs, MPI and install in a custom
location. You can even save that in a shell script to make it even
easier next time. You can also do this kind of thing with <code class="docutils literal"><span class="pre">ccmake</span></code>,
but you should avoid this, because the options set with <code class="docutils literal"><span class="pre">-D</span></code> will not
be able to be changed interactively in that run of <code class="docutils literal"><span class="pre">ccmake</span></code>.</p>
</div>
<div class="section" id="simd-support">
<h4>SIMD support<a class="headerlink" href="#simd-support" title="Permalink to this headline">¶</a></h4>
<p>GROMACS has extensive support for detecting and using the SIMD
capabilities of many modern HPC CPU architectures. If you are building
GROMACS on the same hardware you will run it on, then you don’t need
to read more about this, unless you are getting configuration warnings
you do not understand. By default, the GROMACS build system will
detect the SIMD instruction set supported by the CPU architecture (on
which the configuring is done), and thus pick the best
available SIMD parallelization supported by GROMACS. The build system
will also check that the compiler and linker used also support the
selected SIMD instruction set and issue a fatal error if they
do not.</p>
<p>Valid values are listed below, and the applicable value with the
largest number in the list is generally the one you should choose.
In most cases, choosing an inappropriate higher number will lead
to compiling a binary that will not run. However, on a number of
processor architectures choosing the highest supported value can
lead to performance loss, e.g. on Intel Skylake-X/SP and AMD Zen.</p>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">None</span></code> For use only on an architecture either lacking SIMD,
or to which GROMACS has not yet been ported and none of the
options below are applicable.</li>
<li><code class="docutils literal"><span class="pre">SSE2</span></code> This SIMD instruction set was introduced in Intel
processors in 2001, and AMD in 2003. Essentially all x86
machines in existence have this, so it might be a good choice if
you need to support dinosaur x86 computers too.</li>
<li><code class="docutils literal"><span class="pre">SSE4.1</span></code> Present in all Intel core processors since 2007,
but notably not in AMD Magny-Cours. Still, almost all recent
processors support this, so this can also be considered a good
baseline if you are content with slow simulations and prefer
portability between reasonably modern processors.</li>
<li><code class="docutils literal"><span class="pre">AVX_128_FMA</span></code> AMD Bulldozer, Piledriver (and later Family 15h) processors have this.</li>
<li><code class="docutils literal"><span class="pre">AVX_256</span></code> Intel processors since Sandy Bridge (2011). While this
code will work on the  AMD Bulldozer and Piledriver processors, it is significantly less
efficient than the <code class="docutils literal"><span class="pre">AVX_128_FMA</span></code> choice above - do not be fooled
to assume that 256 is better than 128 in this case.</li>
<li><code class="docutils literal"><span class="pre">AVX2_128</span></code> AMD Zen microarchitecture processors (2017);
it will enable AVX2 with 3-way fused multiply-add instructions.
While the Zen microarchitecture does support 256-bit AVX2 instructions,
hence <code class="docutils literal"><span class="pre">AVX2_256</span></code> is also supported, 128-bit will generally be faster,
in particular when the non-bonded tasks run on the CPU – hence
the default <code class="docutils literal"><span class="pre">AVX2_128</span></code>. With GPU offload however <code class="docutils literal"><span class="pre">AVX2_256</span></code>
can be faster on Zen processors.</li>
<li><code class="docutils literal"><span class="pre">AVX2_256</span></code> Present on Intel Haswell (and later) processors (2013),
and it will also enable Intel 3-way fused multiply-add instructions.</li>
<li><code class="docutils literal"><span class="pre">AVX_512</span></code> Skylake-X desktop and Skylake-SP Xeon processors (2017);
it will generally be fastest on the higher-end desktop and server
processors with two 512-bit fused multiply-add units (e.g. Core i9
and Xeon Gold). However, certain desktop and server models
(e.g. Xeon Bronze and Silver) come with only one AVX512 FMA unit
and therefore on these processors <code class="docutils literal"><span class="pre">AVX2_256</span></code> is faster
(compile- and runtime checks try to inform about such cases).
Additionally, with GPU accelerated runs <code class="docutils literal"><span class="pre">AVX2_256</span></code> can also be
faster on high-end Skylake CPUs with both 512-bit FMA units enabled.</li>
<li><code class="docutils literal"><span class="pre">AVX_512_KNL</span></code> Knights Landing Xeon Phi processors</li>
<li><code class="docutils literal"><span class="pre">IBM_QPX</span></code> BlueGene/Q A2 cores have this.</li>
<li><code class="docutils literal"><span class="pre">Sparc64_HPC_ACE</span></code> Fujitsu machines like the K computer have this.</li>
<li><code class="docutils literal"><span class="pre">IBM_VMX</span></code> Power6 and similar Altivec processors have this.</li>
<li><code class="docutils literal"><span class="pre">IBM_VSX</span></code> Power7, Power8 and later have this.</li>
<li><code class="docutils literal"><span class="pre">ARM_NEON</span></code> 32-bit ARMv7 with NEON support.</li>
<li><code class="docutils literal"><span class="pre">ARM_NEON_ASIMD</span></code> 64-bit ARMv8 and later.</li>
</ol>
<p>The CMake configure system will check that the compiler you have
chosen can target the architecture you have chosen. mdrun will check
further at runtime, so if in doubt, choose the lowest number you
think might work, and see what mdrun says. The configure system also
works around many known issues in many versions of common HPC
compilers.</p>
<p>A further <code class="docutils literal"><span class="pre">GMX_SIMD=Reference</span></code> option exists, which is a special
SIMD-like implementation written in plain C that developers can use
when developing support in GROMACS for new SIMD architectures. It is
not designed for use in production simulations, but if you are using
an architecture with SIMD support to which GROMACS has not yet been
ported, you may wish to try this option instead of the default
<code class="docutils literal"><span class="pre">GMX_SIMD=None</span></code>, as it can often out-perform this when the
auto-vectorization in your compiler does a good job. And post on the
GROMACS mailing lists, because GROMACS can probably be ported for new
SIMD architectures in a few days.</p>
</div>
<div class="section" id="cmake-advanced-options">
<h4>CMake advanced options<a class="headerlink" href="#cmake-advanced-options" title="Permalink to this headline">¶</a></h4>
<p>The options that are displayed in the default view of <code class="docutils literal"><span class="pre">ccmake</span></code> are
ones that we think a reasonable number of users might want to consider
changing. There are a lot more options available, which you can see by
toggling the advanced mode in <code class="docutils literal"><span class="pre">ccmake</span></code> on and off with <code class="docutils literal"><span class="pre">t</span></code>. Even
there, most of the variables that you might want to change have a
<code class="docutils literal"><span class="pre">CMAKE_</span></code> or <code class="docutils literal"><span class="pre">GMX_</span></code> prefix. There are also some options that will be
visible or not according to whether their preconditions are satisfied.</p>
</div>
<div class="section" id="helping-cmake-find-the-right-libraries-headers-or-programs">
<span id="search-for-libraries-headers-or-programs"></span><h4>Helping CMake find the right libraries, headers, or programs<a class="headerlink" href="#helping-cmake-find-the-right-libraries-headers-or-programs" title="Permalink to this headline">¶</a></h4>
<p>If libraries are installed in non-default locations their location can
be specified using the following variables:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">CMAKE_INCLUDE_PATH</span></code> for header files</li>
<li><code class="docutils literal"><span class="pre">CMAKE_LIBRARY_PATH</span></code> for libraries</li>
<li><code class="docutils literal"><span class="pre">CMAKE_PREFIX_PATH</span></code> for header, libraries and binaries
(e.g. <code class="docutils literal"><span class="pre">/usr/local</span></code>).</li>
</ul>
<p>The respective <code class="docutils literal"><span class="pre">include</span></code>, <code class="docutils literal"><span class="pre">lib</span></code>, or <code class="docutils literal"><span class="pre">bin</span></code> is
appended to the path. For each of these variables, a list of paths can
be specified (on Unix, separated with “:”). These can be set as
enviroment variables like:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">CMAKE_PREFIX_PATH</span><span class="o">=</span>/opt/fftw:/opt/cuda cmake ..
</pre></div>
</div>
<p>(assuming <code class="docutils literal"><span class="pre">bash</span></code> shell). Alternatively, these variables are also
<code class="docutils literal"><span class="pre">cmake</span></code> options, so they can be set like
<code class="docutils literal"><span class="pre">-DCMAKE_PREFIX_PATH=/opt/fftw:/opt/cuda</span></code>.</p>
<p>The <code class="docutils literal"><span class="pre">CC</span></code> and <code class="docutils literal"><span class="pre">CXX</span></code> environment variables are also useful
for indicating to <code class="docutils literal"><span class="pre">cmake</span></code> which compilers to use. Similarly,
<code class="docutils literal"><span class="pre">CFLAGS</span></code>/<code class="docutils literal"><span class="pre">CXXFLAGS</span></code> can be used to pass compiler
options, but note that these will be appended to those set by
GROMACS for your build platform and build type. You can customize
some of this with advanced CMake options such as <code class="docutils literal"><span class="pre">CMAKE_C_FLAGS</span></code>
and its relatives.</p>
<p>See also the page on <a class="reference external" href="http://cmake.org/Wiki/CMake_Useful_Variables#Environment_Variables">CMake environment variables</a>.</p>
</div>
<div class="section" id="cuda-gpu-acceleration">
<span id="id2"></span><h4>CUDA GPU acceleration<a class="headerlink" href="#cuda-gpu-acceleration" title="Permalink to this headline">¶</a></h4>
<p>If you have the <a class="reference external" href="http://www.nvidia.com/object/cuda_home_new.html">CUDA</a> Toolkit installed, you can use <code class="docutils literal"><span class="pre">cmake</span></code> with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DGMX_GPU<span class="o">=</span>ON -DCUDA_TOOLKIT_ROOT_DIR<span class="o">=</span>/usr/local/cuda
</pre></div>
</div>
<p>(or whichever path has your installation). In some cases, you might
need to specify manually which of your C++ compilers should be used,
e.g. with the advanced option <code class="docutils literal"><span class="pre">CUDA_HOST_COMPILER</span></code>.</p>
<p>To make it
possible to get best performance from NVIDIA Tesla and Quadro GPUs,
you should install the <a class="reference external" href="https://developer.nvidia.com/gpu-deployment-kit">GPU Deployment Kit</a> and configure
GROMACS to use it by setting the CMake variable
<code class="docutils literal"><span class="pre">-DGPU_DEPLOYMENT_KIT_ROOT_DIR=/path/to/your/kit</span></code>. The NVML support
is most useful if
<code class="docutils literal"><span class="pre">nvidia-smi</span> <span class="pre">--applications-clocks-permission=UNRESTRICTED</span></code> is run
(as root). When application clocks permissions are unrestricted, the
GPU clock speed can be increased automatically, which increases the
GPU kernel performance roughly proportional to the clock
increase. When using GROMACS on suitable GPUs under restricted
permissions, clocks cannot be changed, and in that case informative
log file messages will be produced. Background details can be found at
this <a class="reference external" href="http://devblogs.nvidia.com/parallelforall/increase-performance-gpu-boost-k80-autoboost/">NVIDIA blog post</a>.
NVML support is only available if detected, and may be disabled by
turning off the <code class="docutils literal"><span class="pre">GMX_USE_NVML</span></code> CMake advanced option.</p>
<p>By default, code will be generated for the most common CUDA architectures.
However, to reduce build time and binary size we do not generate code for
every single possible architecture, which in rare cases (say, Tegra systems)
can result in the default build not being able to use some GPUs.
If this happens, or if you want to remove some architectures to reduce
binary size and build time, you can alter the target CUDA architectures.
This can be done either with the <code class="docutils literal"><span class="pre">GMX_CUDA_TARGET_SM</span></code> or
<code class="docutils literal"><span class="pre">GMX_CUDA_TARGET_COMPUTE</span></code> CMake variables, which take a semicolon delimited
string with the two digit suffixes of CUDA (virtual) architectures names, for
instance “35;50;51;52;53;60”. For details, see the “Options for steering GPU
code generation” section of the nvcc man / help or Chapter 6. of the nvcc
manual.</p>
<p>The GPU acceleration has been tested on AMD64/x86-64 platforms with
Linux, Mac OS X and Windows operating systems, but Linux is the
best-tested and supported of these. Linux running on POWER 8, ARM v7 and v8
CPUs also works well.</p>
<p>Experimental support is available for compiling CUDA code, both for host and
device, using clang (version 3.9 or later).
A CUDA toolkit (&gt;= v7.0) is still required but it is used only for GPU device code
generation and to link against the CUDA runtime library.
The clang CUDA support simplifies compilation and provides benefits for development
(e.g. allows the use code sanitizers in CUDA host-code).
Additionally, using clang for both CPU and GPU compilation can be beneficial
to avoid compatibility issues between the GNU toolchain and the CUDA toolkit.
clang for CUDA can be triggered using the <code class="docutils literal"><span class="pre">GMX_CLANG_CUDA=ON</span></code> CMake option.
Target architectures can be selected with  <code class="docutils literal"><span class="pre">GMX_CUDA_TARGET_SM</span></code>,
virtual architecture code is always embedded for all requested architectures
(hence GMX_CUDA_TARGET_COMPUTE is ignored).
Note that this is mainly a developer-oriented feature and it is not recommended
for production use as the performance can be significantly lower than that
of code compiled with nvcc (and it has also received less testing).
However, note that with clang 5.0 the performance gap is significantly narrowed
(at the time of writing, about 20% slower GPU kernels), so this version
could be considered in non performance-critical use-cases.</p>
</div>
<div class="section" id="opencl-gpu-acceleration">
<h4>OpenCL GPU acceleration<a class="headerlink" href="#opencl-gpu-acceleration" title="Permalink to this headline">¶</a></h4>
<p>The primary target of the GROMACS OpenCL support is accelerating simulations
on AMD hardware, both discrete GPUs and APUs (integrated CPU+GPU chips).
The GROMACS OpenCL on NVIDIA GPUs works, but performance
and other limitations make it less practical (for details see the user guide).</p>
<p>To build GROMACS with <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> support enabled, two components are
required: the <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> headers and the wrapper library that acts
as a client driver loader (so-called ICD loader).
The additional, runtime-only dependency is the vendor-specific GPU driver
for the device targeted. This also contains the <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> compiler.
As the GPU compute kernels are compiled  on-demand at run time,
this vendor-specific compiler and driver is not needed for building GROMACS.
The former, compile-time dependencies are standard components,
hence stock versions can be obtained from most Linux distribution
repositories (e.g. <code class="docutils literal"><span class="pre">opencl-headers</span></code> and <code class="docutils literal"><span class="pre">ocl-icd-libopencl1</span></code> on Debian/Ubuntu).
Only the compatibility with the required <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> version 1.1
needs to be ensured.
Alternatively, the headers and library can also be obtained from vendor SDKs
(e.g. <a class="reference external" href="http://developer.amd.com/appsdk">from AMD</a>),
which must be installed in a path found in <code class="docutils literal"><span class="pre">CMAKE_PREFIX_PATH</span></code> (or via the environment
variables <code class="docutils literal"><span class="pre">AMDAPPSDKROOT</span></code> or <code class="docutils literal"><span class="pre">CUDA_PATH</span></code>).</p>
<p>To trigger an <a class="reference external" href="https://www.khronos.org/opencl/">OpenCL</a> build the following CMake flags must be set</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DGMX_GPU<span class="o">=</span>ON -DGMX_USE_OPENCL<span class="o">=</span>ON
</pre></div>
</div>
<p>On Mac OS, an AMD GPU can be used only with OS version 10.10.4 and
higher; earlier OS versions are known to run incorrectly.</p>
</div>
<div class="section" id="static-linking">
<h4>Static linking<a class="headerlink" href="#static-linking" title="Permalink to this headline">¶</a></h4>
<p>Dynamic linking of the GROMACS executables will lead to a
smaller disk footprint when installed, and so is the default on
platforms where we believe it has been tested repeatedly and found to work.
In general, this includes Linux, Windows, Mac OS X and BSD systems.
Static binaries take more space, but on some hardware and/or under
some conditions they are necessary, most commonly when you are running a parallel
simulation using MPI libraries (e.g. BlueGene, Cray).</p>
<ul class="simple">
<li>To link GROMACS binaries statically against the internal GROMACS
libraries, set <code class="docutils literal"><span class="pre">-DBUILD_SHARED_LIBS=OFF</span></code>.</li>
<li>To link statically against external (non-system) libraries as well,
set <code class="docutils literal"><span class="pre">-DGMX_PREFER_STATIC_LIBS=ON</span></code>. Note, that in
general <code class="docutils literal"><span class="pre">cmake</span></code> picks up whatever is available, so this option only
instructs <code class="docutils literal"><span class="pre">cmake</span></code> to prefer static libraries when both static and
shared are available. If no static version of an external library is
available, even when the aforementioned option is <code class="docutils literal"><span class="pre">ON</span></code>, the shared
library will be used. Also note that the resulting binaries will
still be dynamically linked against system libraries on platforms
where that is the default. To use static system libraries,
additional compiler/linker flags are necessary, e.g. <code class="docutils literal"><span class="pre">-static-libgcc</span>
<span class="pre">-static-libstdc++</span></code>.</li>
<li>To attempt to link a fully static binary set
<code class="docutils literal"><span class="pre">-DGMX_BUILD_SHARED_EXE=OFF</span></code>. This will prevent CMake from explicitly
setting any dynamic linking flags. This option also sets
<code class="docutils literal"><span class="pre">-DBUILD_SHARED_LIBS=OFF</span></code> and <code class="docutils literal"><span class="pre">-DGMX_PREFER_STATIC_LIBS=ON</span></code> by
default, but the above caveats apply. For compilers which don’t
default to static linking, the required flags have to be specified. On
Linux, this is usually <code class="docutils literal"><span class="pre">CFLAGS=-static</span> <span class="pre">CXXFLAGS=-static</span></code>.</li>
</ul>
</div>
<div class="section" id="portability-aspects">
<h4>Portability aspects<a class="headerlink" href="#portability-aspects" title="Permalink to this headline">¶</a></h4>
<p>A GROMACS build will normally not be portable, not even across
hardware with the same base instruction set, like x86. Non-portable
hardware-specific optimizations are selected at configure-time, such
as the SIMD instruction set used in the compute kernels. This
selection will be done by the build system based on the capabilities
of the build host machine or otherwise specified to <code class="docutils literal"><span class="pre">cmake</span></code> during
configuration.</p>
<p>Often it is possible to ensure portability by choosing the least
common denominator of SIMD support, e.g. SSE2 for x86, and ensuring
the you use <code class="docutils literal"><span class="pre">cmake</span> <span class="pre">-DGMX_USE_RDTSCP=off</span></code> if any of the target CPU
architectures does not support the <code class="docutils literal"><span class="pre">RDTSCP</span></code> instruction.  However, we
discourage attempts to use a single GROMACS installation when the
execution environment is heterogeneous, such as a mix of AVX and
earlier hardware, because this will lead to programs (especially
mdrun) that run slowly on the new hardware. Building two full
installations and locally managing how to call the correct one
(e.g. using a module system) is the recommended
approach. Alternatively, as at the moment the GROMACS tools do not
make strong use of SIMD acceleration, it can be convenient to create
an installation with tools portable across different x86 machines, but
with separate mdrun binaries for each architecture. To achieve this,
one can first build a full installation with the
least-common-denominator SIMD instruction set, e.g. <code class="docutils literal"><span class="pre">-DGMX_SIMD=SSE2</span></code>,
then build separate mdrun binaries for each architecture present in
the heterogeneous environment. By using custom binary and library
suffixes for the mdrun-only builds, these can be installed to the
same location as the “generic” tools installation.
<a class="reference internal" href="#building-just-the-mdrun-binary">Building just the mdrun binary</a> is possible by setting the
<code class="docutils literal"><span class="pre">-DGMX_BUILD_MDRUN_ONLY=ON</span></code> option.</p>
</div>
<div class="section" id="linear-algebra-libraries">
<h4>Linear algebra libraries<a class="headerlink" href="#linear-algebra-libraries" title="Permalink to this headline">¶</a></h4>
<p>As mentioned above, sometimes vendor BLAS and LAPACK libraries
can provide performance enhancements for GROMACS when doing
normal-mode analysis or covariance analysis. For simplicity, the text
below will refer only to BLAS, but the same options are available
for LAPACK. By default, CMake will search for BLAS, use it if it
is found, and otherwise fall back on a version of BLAS internal to
GROMACS. The <code class="docutils literal"><span class="pre">cmake</span></code> option <code class="docutils literal"><span class="pre">-DGMX_EXTERNAL_BLAS=on</span></code> will be set
accordingly. The internal versions are fine for normal use. If you
need to specify a non-standard path to search, use
<code class="docutils literal"><span class="pre">-DCMAKE_PREFIX_PATH=/path/to/search</span></code>. If you need to specify a
library with a non-standard name (e.g. ESSL on AIX or BlueGene), then
set <code class="docutils literal"><span class="pre">-DGMX_BLAS_USER=/path/to/reach/lib/libwhatever.a</span></code>.</p>
<p>If you are using Intel <a class="reference external" href="https://software.intel.com/en-us/intel-mkl">MKL</a> for FFT, then the BLAS and
LAPACK it provides are used automatically. This could be
over-ridden with <code class="docutils literal"><span class="pre">GMX_BLAS_USER</span></code>, etc.</p>
<p>On Apple platforms where the Accelerate Framework is available, these
will be automatically used for BLAS and LAPACK. This could be
over-ridden with <code class="docutils literal"><span class="pre">GMX_BLAS_USER</span></code>, etc.</p>
</div>
<div class="section" id="changing-the-names-of-gromacs-binaries-and-libraries">
<h4>Changing the names of GROMACS binaries and libraries<a class="headerlink" href="#changing-the-names-of-gromacs-binaries-and-libraries" title="Permalink to this headline">¶</a></h4>
<p>It is sometimes convenient to have different versions of the same
GROMACS programs installed. The most common use cases have been single
and double precision, and with and without MPI. This mechanism can
also be used to install side-by-side multiple versions of mdrun
optimized for different CPU architectures, as mentioned previously.</p>
<p>By default, GROMACS will suffix programs and libraries for such builds
with <code class="docutils literal"><span class="pre">_d</span></code> for double precision and/or <code class="docutils literal"><span class="pre">_mpi</span></code> for MPI (and nothing
otherwise). This can be controlled manually with <code class="docutils literal"><span class="pre">GMX_DEFAULT_SUFFIX</span>
<span class="pre">(ON/OFF)</span></code>, <code class="docutils literal"><span class="pre">GMX_BINARY_SUFFIX</span></code> (takes a string) and <code class="docutils literal"><span class="pre">GMX_LIBS_SUFFIX</span></code>
(also takes a string). For instance, to set a custom suffix for
programs and libraries, one might specify:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DGMX_DEFAULT_SUFFIX<span class="o">=</span>OFF -DGMX_BINARY_SUFFIX<span class="o">=</span>_mod -DGMX_LIBS_SUFFIX<span class="o">=</span>_mod
</pre></div>
</div>
<p>Thus the names of all programs and libraries will be appended with
<code class="docutils literal"><span class="pre">_mod</span></code>.</p>
</div>
<div class="section" id="changing-installation-tree-structure">
<h4>Changing installation tree structure<a class="headerlink" href="#changing-installation-tree-structure" title="Permalink to this headline">¶</a></h4>
<p>By default, a few different directories under <code class="docutils literal"><span class="pre">CMAKE_INSTALL_PREFIX</span></code> are used
when when GROMACS is installed. Some of these can be changed, which is mainly
useful for packaging GROMACS for various distributions. The directories are
listed below, with additional notes about some of them. Unless otherwise noted,
the directories can be renamed by editing the installation paths in the main
CMakeLists.txt.</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">bin/</span></code></dt>
<dd>The standard location for executables and some scripts.
Some of the scripts hardcode the absolute installation prefix, which needs
to be changed if the scripts are relocated.
The name of the directory can be changed using <code class="docutils literal"><span class="pre">CMAKE_INSTALL_BINDIR</span></code> CMake
variable.</dd>
<dt><code class="docutils literal"><span class="pre">include/gromacs/</span></code></dt>
<dd>The standard location for installed headers.</dd>
<dt><code class="docutils literal"><span class="pre">lib/</span></code></dt>
<dd>The standard location for libraries. The default depends on the system, and
is determined by CMake.
The name of the directory can be changed using <code class="docutils literal"><span class="pre">CMAKE_INSTALL_LIBDIR</span></code> CMake
variable.</dd>
<dt><code class="docutils literal"><span class="pre">lib/pkgconfig/</span></code></dt>
<dd>Information about the installed <code class="docutils literal"><span class="pre">libgromacs</span></code> library for <code class="docutils literal"><span class="pre">pkg-config</span></code> is
installed here.  The <code class="docutils literal"><span class="pre">lib/</span></code> part adapts to the installation location of the
libraries.  The installed files contain the installation prefix as absolute
paths.</dd>
<dt><code class="docutils literal"><span class="pre">share/cmake/</span></code></dt>
<dd>CMake package configuration files are installed here.</dd>
<dt><code class="docutils literal"><span class="pre">share/gromacs/</span></code></dt>
<dd>Various data files and some documentation go here. The first part can
be changed using <code class="docutils literal"><span class="pre">CMAKE_INSTALL_DATADIR</span></code>, and the second by using
<code class="docutils literal"><span class="pre">GMX_INSTALL_DATASUBDIR</span></code> Using these CMake variables is the preferred
way of changing the installation path for
<code class="docutils literal"><span class="pre">share/gromacs/top/</span></code>, since the path to this directory is built into
<code class="docutils literal"><span class="pre">libgromacs</span></code> as well as some scripts, both as a relative and as an absolute
path (the latter as a fallback if everything else fails).</dd>
<dt><code class="docutils literal"><span class="pre">share/man/</span></code></dt>
<dd>Installed man pages go here.</dd>
</dl>
</div>
</div>
<div class="section" id="compiling-and-linking">
<h3>Compiling and linking<a class="headerlink" href="#compiling-and-linking" title="Permalink to this headline">¶</a></h3>
<p>Once you have configured with <code class="docutils literal"><span class="pre">cmake</span></code>, you can build GROMACS with <code class="docutils literal"><span class="pre">make</span></code>.
It is expected that this will always complete successfully, and
give few or no warnings. The CMake-time tests GROMACS makes on the settings
you choose are pretty extensive, but there are probably a few cases we
have not thought of yet. Search the web first for solutions to
problems, but if you need help, ask on gmx-users, being sure to
provide as much information as possible about what you did, the system
you are building on, and what went wrong. This may mean scrolling back
a long way through the output of <code class="docutils literal"><span class="pre">make</span></code> to find the first error
message!</p>
<p>If you have a multi-core or multi-CPU machine with <code class="docutils literal"><span class="pre">N</span></code>
processors, then using</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>make -j N
</pre></div>
</div>
<p>will generally speed things up by quite a bit. Other build generator systems
supported by <code class="docutils literal"><span class="pre">cmake</span></code> (e.g. <code class="docutils literal"><span class="pre">ninja</span></code>) also work well.</p>
<div class="section" id="building-only-mdrun">
<span id="building-just-the-mdrun-binary"></span><h4>Building only mdrun<a class="headerlink" href="#building-only-mdrun" title="Permalink to this headline">¶</a></h4>
<p>This is now supported with the <code class="docutils literal"><span class="pre">cmake</span></code> option
<code class="docutils literal"><span class="pre">-DGMX_BUILD_MDRUN_ONLY=ON</span></code>, which will build a different version of
<code class="docutils literal"><span class="pre">libgromacs</span></code> and the <code class="docutils literal"><span class="pre">mdrun</span></code> program.
Naturally, now <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code> installs only those
products. By default, mdrun-only builds will default to static linking
against GROMACS libraries, because this is generally a good idea for
the targets for which an mdrun-only build is desirable.</p>
</div>
</div>
<div class="section" id="installing-gromacs">
<h3>Installing GROMACS<a class="headerlink" href="#installing-gromacs" title="Permalink to this headline">¶</a></h3>
<p>Finally, <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code> will install GROMACS in the
directory given in <code class="docutils literal"><span class="pre">CMAKE_INSTALL_PREFIX</span></code>. If this is a system
directory, then you will need permission to write there, and you
should use super-user privileges only for <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code> and
not the whole procedure.</p>
</div>
<div class="section" id="getting-access-to-gromacs-after-installation">
<span id="getting-access-to-gromacs"></span><h3>Getting access to GROMACS after installation<a class="headerlink" href="#getting-access-to-gromacs-after-installation" title="Permalink to this headline">¶</a></h3>
<p>GROMACS installs the script <code class="docutils literal"><span class="pre">GMXRC</span></code> in the <code class="docutils literal"><span class="pre">bin</span></code>
subdirectory of the installation directory
(e.g. <code class="docutils literal"><span class="pre">/usr/local/gromacs/bin/GMXRC</span></code>), which you should source
from your shell:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nb">source</span> /your/installation/prefix/here/bin/GMXRC
</pre></div>
</div>
<p>It will detect what kind of shell you are running and set up your
environment for using GROMACS. You may wish to arrange for your
login scripts to do this automatically; please search the web for
instructions on how to do this for your shell.</p>
<p>Many of the GROMACS programs rely on data installed in the
<code class="docutils literal"><span class="pre">share/gromacs</span></code> subdirectory of the installation directory. By
default, the programs will use the environment variables set in the
<code class="docutils literal"><span class="pre">GMXRC</span></code> script, and if this is not available they will try to guess the
path based on their own location.  This usually works well unless you
change the names of directories inside the install tree. If you still
need to do that, you might want to recompile with the new install
location properly set, or edit the <code class="docutils literal"><span class="pre">GMXRC</span></code> script.</p>
</div>
<div class="section" id="testing-gromacs-for-correctness">
<h3>Testing GROMACS for correctness<a class="headerlink" href="#testing-gromacs-for-correctness" title="Permalink to this headline">¶</a></h3>
<p>Since 2011, the GROMACS development uses an automated system where
every new code change is subject to regression testing on a number of
platforms and software combinations. While this improves
reliability quite a lot, not everything is tested, and since we
increasingly rely on cutting edge compiler features there is
non-negligible risk that the default compiler on your system could
have bugs. We have tried our best to test and refuse to use known bad
versions in <code class="docutils literal"><span class="pre">cmake</span></code>, but we strongly recommend that you run through
the tests yourself. It only takes a few minutes, after which you can
trust your build.</p>
<p>The simplest way to run the checks is to build GROMACS with
<code class="docutils literal"><span class="pre">-DREGRESSIONTEST_DOWNLOAD</span></code>, and run <code class="docutils literal"><span class="pre">make</span> <span class="pre">check</span></code>.
GROMACS will automatically download and run the tests for you.
Alternatively, you can download and unpack the GROMACS
regression test suite <a class="reference external" href="http://gerrit.gromacs.org/download/regressiontests-2018.3.tar.gz">http://gerrit.gromacs.org/download/regressiontests-2018.3.tar.gz</a> tarball yourself
and use the advanced <code class="docutils literal"><span class="pre">cmake</span></code> option <code class="docutils literal"><span class="pre">REGRESSIONTEST_PATH</span></code> to
specify the path to the unpacked tarball, which will then be used for
testing. If the above does not work, then please read on.</p>
<p>The regression tests are also available from the <a class="reference external" href="../download.html">download</a> section.
Once you have downloaded them, unpack the tarball, source
<code class="docutils literal"><span class="pre">GMXRC</span></code> as described above, and run <code class="docutils literal"><span class="pre">./gmxtest.pl</span> <span class="pre">all</span></code>
inside the regression tests folder. You can find more options
(e.g. adding <code class="docutils literal"><span class="pre">double</span></code> when using double precision, or
<code class="docutils literal"><span class="pre">-only</span> <span class="pre">expanded</span></code> to run just the tests whose names match
“expanded”) if you just execute the script without options.</p>
<p>Hopefully, you will get a report that all tests have passed. If there
are individual failed tests it could be a sign of a compiler bug, or
that a tolerance is just a tiny bit too tight. Check the output files
the script directs you too, and try a different or newer compiler if
the errors appear to be real. If you cannot get it to pass the
regression tests, you might try dropping a line to the gmx-users
mailing list, but then you should include a detailed description of
your hardware, and the output of <code class="docutils literal"><span class="pre">gmx</span> <span class="pre">mdrun</span> <span class="pre">-version</span></code> (which contains
valuable diagnostic information in the header).</p>
<p>A build with <code class="docutils literal"><span class="pre">-DGMX_BUILD_MDRUN_ONLY</span></code> cannot be tested with
<code class="docutils literal"><span class="pre">make</span> <span class="pre">check</span></code> from the build tree, because most of the tests
require a full build to run things like <code class="docutils literal"><span class="pre">grompp</span></code>. To test such an
mdrun fully requires installing it to the same location as a normal
build of GROMACS, downloading the regression tests tarball manually
as described above, sourcing the correct <code class="docutils literal"><span class="pre">GMXRC</span></code> and running the
perl script manually. For example, from your GROMACS source
directory:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir build-normal
<span class="nb">cd</span> build-normal
cmake .. -DCMAKE_INSTALL_PREFIX<span class="o">=</span>/your/installation/prefix/here
make -j <span class="m">4</span>
make install
<span class="nb">cd</span> ..
mkdir build-mdrun-only
<span class="nb">cd</span> build-mdrun-only
cmake .. -DGMX_MPI<span class="o">=</span>ON -DGMX_GPU<span class="o">=</span>ON -DGMX_BUILD_MDRUN_ONLY<span class="o">=</span>ON -DCMAKE_INSTALL_PREFIX<span class="o">=</span>/your/installation/prefix/here
make -j <span class="m">4</span>
make install
<span class="nb">cd</span> /to/your/unpacked/regressiontests
<span class="nb">source</span> /your/installation/prefix/here/bin/GMXRC
./gmxtest.pl all -np <span class="m">2</span>
</pre></div>
</div>
<p>If your mdrun program has been suffixed in a non-standard way, then
the <code class="docutils literal"><span class="pre">./gmxtest.pl</span> <span class="pre">-mdrun</span></code> option will let you specify that name to the
test machinery. You can use <code class="docutils literal"><span class="pre">./gmxtest.pl</span> <span class="pre">-double</span></code> to test the
double-precision version. You can use <code class="docutils literal"><span class="pre">./gmxtest.pl</span> <span class="pre">-crosscompiling</span></code>
to stop the test harness attempting to check that the programs can
be run. You can use <code class="docutils literal"><span class="pre">./gmxtest.pl</span> <span class="pre">-mpirun</span> <span class="pre">srun</span></code> if your command to
run an MPI program is called <code class="docutils literal"><span class="pre">srun</span></code>.</p>
<p>The <code class="docutils literal"><span class="pre">make</span> <span class="pre">check</span></code> target also runs integration-style tests that may run
with MPI if <code class="docutils literal"><span class="pre">GMX_MPI=ON</span></code> was set. To make these work with various possible
MPI libraries, you may need to
set the CMake variables <code class="docutils literal"><span class="pre">MPIEXEC</span></code>, <code class="docutils literal"><span class="pre">MPIEXEC_NUMPROC_FLAG</span></code>,
<code class="docutils literal"><span class="pre">MPIEXEC_PREFLAGS</span></code> and <code class="docutils literal"><span class="pre">MPIEXEC_POSTFLAGS</span></code> so that
<code class="docutils literal"><span class="pre">mdrun-mpi-test_mpi</span></code> would run on multiple ranks via the shell command</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="si">${</span><span class="nv">MPIEXEC</span><span class="si">}</span> <span class="si">${</span><span class="nv">MPIEXEC_NUMPROC_FLAG</span><span class="si">}</span> <span class="si">${</span><span class="nv">NUMPROC</span><span class="si">}</span> <span class="si">${</span><span class="nv">MPIEXEC_PREFLAGS</span><span class="si">}</span> <span class="se">\</span>
      mdrun-mpi-test_mpi <span class="si">${</span><span class="nv">MPIEXEC_POSTFLAGS</span><span class="si">}</span> -otherflags
</pre></div>
</div>
<p>A typical example for SLURM is</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DGMX_MPI<span class="o">=</span>on -DMPIEXEC<span class="o">=</span>srun -DMPIEXEC_NUMPROC_FLAG<span class="o">=</span>-n -DMPIEXEC_PREFLAGS<span class="o">=</span> -DMPIEXEC_POSTFLAGS<span class="o">=</span>
</pre></div>
</div>
</div>
<div class="section" id="testing-gromacs-for-performance">
<h3>Testing GROMACS for performance<a class="headerlink" href="#testing-gromacs-for-performance" title="Permalink to this headline">¶</a></h3>
<p>We are still working on a set of benchmark systems for testing
the performance of GROMACS. Until that is ready, we recommend that
you try a few different parallelization options, and experiment with
tools such as <code class="docutils literal"><span class="pre">gmx</span> <span class="pre">tune_pme</span></code>.</p>
</div>
<div class="section" id="having-difficulty">
<h3>Having difficulty?<a class="headerlink" href="#having-difficulty" title="Permalink to this headline">¶</a></h3>
<p>You are not alone - this can be a complex task! If you encounter a
problem with installing GROMACS, then there are a number of
locations where you can find assistance. It is recommended that you
follow these steps to find the solution:</p>
<ol class="arabic simple">
<li>Read the installation instructions again, taking note that you
have followed each and every step correctly.</li>
<li>Search the GROMACS <a class="reference external" href="http://www.gromacs.org">webpage</a> and users emailing list for information
on the error. Adding
<code class="docutils literal"><span class="pre">site:https://mailman-1.sys.kth.se/pipermail/gromacs.org_gmx-users</span></code>
to a Google search may help filter better results.</li>
<li>Search the internet using a search engine such as Google.</li>
<li>Post to the GROMACS users emailing list gmx-users for
assistance. Be sure to give a full description of what you have
done and why you think it did not work. Give details about the
system on which you are installing.  Copy and paste your command
line and as much of the output as you think might be relevant -
certainly from the first indication of a problem. In particular,
please try to include at least the header from the mdrun logfile,
and preferably the entire file.  People who might volunteer to help
you do not have time to ask you interactive detailed follow-up
questions, so you will get an answer faster if you provide as much
information as you think could possibly help. High quality bug
reports tend to receive rapid high quality answers.</li>
</ol>
</div>
</div>
<div class="section" id="special-instructions-for-some-platforms">
<span id="gmx-special-build"></span><h2>Special instructions for some platforms<a class="headerlink" href="#special-instructions-for-some-platforms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="building-on-windows">
<h3>Building on Windows<a class="headerlink" href="#building-on-windows" title="Permalink to this headline">¶</a></h3>
<p>Building on Windows using native compilers is rather similar to
building on Unix, so please start by reading the above. Then, download
and unpack the GROMACS source archive. Make a folder in which to do
the out-of-source build of GROMACS. For example, make it within the
folder unpacked from the source archive, and call it <code class="docutils literal"><span class="pre">build-gromacs</span></code>.</p>
<p>For CMake, you can either use the graphical user interface provided on
Windows, or you can use a command line shell with instructions similar
to the UNIX ones above. If you open a shell from within your IDE
(e.g. Microsoft Visual Studio), it will configure the environment for
you, but you might need to tweak this in order to get either a 32-bit
or 64-bit build environment. The latter provides the fastest
executable. If you use a normal Windows command shell, then you will
need to either set up the environment to find your compilers and
libraries yourself, or run the <code class="docutils literal"><span class="pre">vcvarsall.bat</span></code> batch script provided
by MSVC (just like sourcing a bash script under Unix).</p>
<p>With the graphical user interface, you will be asked about what
compilers to use at the initial configuration stage, and if you use
the command line they can be set in a similar way as under UNIX.</p>
<p>Unfortunately <code class="docutils literal"><span class="pre">-DGMX_BUILD_OWN_FFTW=ON</span></code> (see <a class="reference internal" href="#using-fftw">Using FFTW</a>) does not
work on Windows, because there is no supported way to build FFTW on
Windows. You can either build FFTW some other way (e.g. MinGW), or
use the built-in fftpack (which may be slow), or <a class="reference internal" href="#using-mkl">using MKL</a>.</p>
<p>For the build, you can either load the generated solutions file into
e.g. Visual Studio, or use the command line with <code class="docutils literal"><span class="pre">cmake</span> <span class="pre">--build</span></code> so
the right tools get used.</p>
</div>
<div class="section" id="building-on-cray">
<h3>Building on Cray<a class="headerlink" href="#building-on-cray" title="Permalink to this headline">¶</a></h3>
<p>GROMACS builds mostly out of the box on modern Cray machines, but
you may need to specify the use of static binaries with
<code class="docutils literal"><span class="pre">-DGMX_BUILD_SHARED_EXE=off</span></code>, and you may need to set the F77
environmental variable to <code class="docutils literal"><span class="pre">ftn</span></code> when compiling FFTW.
The ARM ThunderX2 Cray XC50 machines differ only in that the recommended
compiler is the ARM HPC Compiler (<code class="docutils literal"><span class="pre">armclang</span></code>).</p>
</div>
<div class="section" id="building-on-solaris">
<h3>Building on Solaris<a class="headerlink" href="#building-on-solaris" title="Permalink to this headline">¶</a></h3>
<p>The built-in GROMACS processor detection does not work on Solaris,
so it is strongly recommended that you build GROMACS with
<code class="docutils literal"><span class="pre">-DGMX_HWLOC=on</span></code> and ensure that the <code class="docutils literal"><span class="pre">CMAKE_PREFIX_PATH</span></code> includes
the path where the hwloc headers and libraries can be found. At least
version 1.11.8 of hwloc is recommended.</p>
<p>Oracle Developer Studio is not a currently supported compiler (and
does not currently compile GROMACS correctly, perhaps because the
thread-MPI atomics are incorrectly implemented in GROMACS).</p>
</div>
<div class="section" id="building-on-bluegene">
<h3>Building on BlueGene<a class="headerlink" href="#building-on-bluegene" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bluegene-q">
<h4>BlueGene/Q<a class="headerlink" href="#bluegene-q" title="Permalink to this headline">¶</a></h4>
<p>There is currently native acceleration on this platform for the Verlet
cut-off scheme. There are no plans to provide accelerated kernels for
the group cut-off scheme, but the default plain C kernels will work
(slowly).</p>
<p>Only the bgclang compiler is supported, because it is the only
availble C++11 compiler. Only static linking is supported.</p>
<p>Computation on BlueGene floating-point units is always done in
double-precision. However, mixed-precision builds of GROMACS are still
normal and encouraged since they use cache more efficiently.</p>
<p>You need to arrange for FFTW to be installed correctly, following the
above instructions. You may prefer to configure FFTW with
<code class="docutils literal"><span class="pre">--disable-fortran</span></code> to avoid complications.</p>
<p>MPI wrapper compilers should be used for compiling and linking. The
MPI wrapper compilers can make it awkward to
attempt to use IBM’s optimized BLAS/LAPACK called ESSL (see the
section on <a class="reference internal" href="#linear-algebra-libraries">linear algebra libraries</a>. Since mdrun is the only part
of GROMACS that should normally run on the compute nodes, and there is
nearly no need for linear algebra support for mdrun, it is recommended
to use the GROMACS built-in linear algebra routines - this is never
a problem for normal simulations.</p>
<p>The recommended configuration is to use</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DCMAKE_C_COMPILER<span class="o">=</span>mpicc <span class="se">\</span>
         -DCMAKE_CXX_COMPILER<span class="o">=</span>mpicxx <span class="se">\</span>
         -DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>Platform/BlueGeneQ-static-bgclang-CXX.cmake <span class="se">\</span>
         -DCMAKE_PREFIX_PATH<span class="o">=</span>/your/fftw/installation/prefix <span class="se">\</span>
         -DGMX_MPI<span class="o">=</span>ON <span class="se">\</span>
         -DGMX_BUILD_MDRUN_ONLY<span class="o">=</span>ON
make
make install
</pre></div>
</div>
<p>which will build a statically-linked MPI-enabled mdrun for the compute
nodes. Otherwise, GROMACS default configuration
behaviour applies.</p>
<p>It is possible to configure and make the remaining GROMACS tools with
the compute-node toolchain, but as none of those tools are MPI-aware,
this would not normally
be useful. Instead, users should plan to run these on the login node,
and perform a separate GROMACS installation for that, using the login
node’s toolchain - not the above platform file, or any other
compute-node toolchain. This may require requesting an up-to-date
gcc or clang toolchain for the front end.</p>
<p>Note that only the MPI build is available for the compute-node
toolchains. The GROMACS thread-MPI or no-MPI builds are not useful at
all on BlueGene/Q.</p>
</div>
<div class="section" id="fujitsu-primehpc">
<h4>Fujitsu PRIMEHPC<a class="headerlink" href="#fujitsu-primehpc" title="Permalink to this headline">¶</a></h4>
<p>This is the architecture of the K computer, which uses Fujitsu
Sparc64VIIIfx chips. On this platform, GROMACS has
accelerated group kernels using the HPC-ACE instructions, no
accelerated Verlet kernels, and a custom build toolchain. Since this
particular chip only does double precision SIMD, the default setup
is to build GROMACS in double. Since most users only need single, we have added
an option GMX_RELAXED_DOUBLE_PRECISION to accept single precision square root
accuracy in the group kernels; unless you know that you really need 15 digits
of accuracy in each individual force, we strongly recommend you use this. Note
that all summation and other operations are still done in double.</p>
<p>The recommended configuration is to use</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>Toolchain-Fujitsu-Sparc64-mpi.cmake <span class="se">\</span>
         -DCMAKE_PREFIX_PATH<span class="o">=</span>/your/fftw/installation/prefix <span class="se">\</span>
         -DCMAKE_INSTALL_PREFIX<span class="o">=</span>/where/gromacs/should/be/installed <span class="se">\</span>
         -DGMX_MPI<span class="o">=</span>ON <span class="se">\</span>
         -DGMX_BUILD_MDRUN_ONLY<span class="o">=</span>ON <span class="se">\</span>
         -DGMX_RELAXED_DOUBLE_PRECISION<span class="o">=</span>ON
make
make install
</pre></div>
</div>
</div>
<div class="section" id="intel-xeon-phi">
<h4>Intel Xeon Phi<a class="headerlink" href="#intel-xeon-phi" title="Permalink to this headline">¶</a></h4>
<p>Xeon Phi processors, hosted or self-hosted, are supported.
Only symmetric (aka native) mode is supported on Knights Corner. The
performance depends among other factors on the system size, and for
now the performance might not be faster than CPUs. When building for it,
the recommended configuration is</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>cmake .. -DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>Platform/XeonPhi
make
make install
</pre></div>
</div>
<p>The Knights Landing-based Xeon Phi processors behave like standard x86 nodes,
but support a special SIMD instruction set. When cross-compiling for such nodes,
use the <code class="docutils literal"><span class="pre">AVX_512_KNL</span></code> SIMD flavor.
Knights Landing processors support so-called “clustering modes” which
allow reconfiguring the memory subsystem for lower latency. GROMACS can
benefit from the quadrant or SNC clustering modes.
Care needs to be taken to correctly pin threads. In particular, threads of
an MPI rank should not cross cluster and NUMA boundaries.
In addition to the main DRAM memory, Knights Landing has a high-bandwidth
stacked memory called MCDRAM. Using it offers performance benefits if
it is ensured that <code class="docutils literal"><span class="pre">mdrun</span></code> runs entirely from this memory; to do so
it is recommended that MCDRAM is configured in “Flat mode” and <code class="docutils literal"><span class="pre">mdrun</span></code> is
bound to the appropriate NUMA node (use e.g. <code class="docutils literal"><span class="pre">numactl</span> <span class="pre">--membind</span> <span class="pre">1</span></code> with
quadrant clustering mode).</p>
</div>
</div>
</div>
<div class="section" id="tested-platforms">
<h2>Tested platforms<a class="headerlink" href="#tested-platforms" title="Permalink to this headline">¶</a></h2>
<p>While it is our best belief that GROMACS will build and run pretty
much everywhere, it is important that we tell you where we really know
it works because we have tested it. We do test on Linux, Windows, and
Mac with a range of compilers and libraries for a range of our
configuration options. Every commit in our git source code repository
is currently tested on x86 with a number of gcc versions ranging from 4.8.1
through 7, versions 16 and 18 of the Intel compiler, and Clang
versions 3.4 through 5. For this, we use a variety of GNU/Linux
flavors and versions as well as recent versions of Windows. Under
Windows, we test both MSVC 2015 and version 16 of the Intel compiler.
For details, you can
have a look at the <a class="reference external" href="http://jenkins.gromacs.org">continuous integration server used by GROMACS</a>,
which runs <a class="reference external" href="http://jenkins-ci.org">Jenkins</a>.</p>
<p>We test irregularly on ARM v7, ARM v8, BlueGene/Q, Cray, Fujitsu
PRIMEHPC, Power8, Google Native Client and other environments, and
with other compilers and compiler versions, too.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Installation guide</a><ul>
<li><a class="reference internal" href="#introduction-to-building-gromacs">Introduction to building GROMACS</a><ul>
<li><a class="reference internal" href="#quick-and-dirty-installation">Quick and dirty installation</a></li>
<li><a class="reference internal" href="#quick-and-dirty-cluster-installation">Quick and dirty cluster installation</a></li>
<li><a class="reference internal" href="#typical-installation">Typical installation</a></li>
<li><a class="reference internal" href="#building-older-versions">Building older versions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li><a class="reference internal" href="#platform">Platform</a></li>
<li><a class="reference internal" href="#compiler">Compiler</a></li>
<li><a class="reference internal" href="#compiling-with-parallelization-options">Compiling with parallelization options</a><ul>
<li><a class="reference internal" href="#gpu-support">GPU support</a></li>
<li><a class="reference internal" href="#mpi-support">MPI support</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cmake">CMake</a></li>
<li><a class="reference internal" href="#fast-fourier-transform-library">Fast Fourier Transform library</a><ul>
<li><a class="reference internal" href="#using-fftw">Using FFTW</a></li>
<li><a class="reference internal" href="#using-mkl">Using MKL</a></li>
</ul>
</li>
<li><a class="reference internal" href="#other-optional-build-components">Other optional build components</a></li>
</ul>
</li>
<li><a class="reference internal" href="#doing-a-build-of-gromacs">Doing a build of GROMACS</a><ul>
<li><a class="reference internal" href="#configuring-with-cmake">Configuring with CMake</a><ul>
<li><a class="reference internal" href="#where-to-install-gromacs">Where to install GROMACS</a></li>
<li><a class="reference internal" href="#using-cmake-command-line-options">Using CMake command-line options</a></li>
<li><a class="reference internal" href="#simd-support">SIMD support</a></li>
<li><a class="reference internal" href="#cmake-advanced-options">CMake advanced options</a></li>
<li><a class="reference internal" href="#helping-cmake-find-the-right-libraries-headers-or-programs">Helping CMake find the right libraries, headers, or programs</a></li>
<li><a class="reference internal" href="#cuda-gpu-acceleration">CUDA GPU acceleration</a></li>
<li><a class="reference internal" href="#opencl-gpu-acceleration">OpenCL GPU acceleration</a></li>
<li><a class="reference internal" href="#static-linking">Static linking</a></li>
<li><a class="reference internal" href="#portability-aspects">Portability aspects</a></li>
<li><a class="reference internal" href="#linear-algebra-libraries">Linear algebra libraries</a></li>
<li><a class="reference internal" href="#changing-the-names-of-gromacs-binaries-and-libraries">Changing the names of GROMACS binaries and libraries</a></li>
<li><a class="reference internal" href="#changing-installation-tree-structure">Changing installation tree structure</a></li>
</ul>
</li>
<li><a class="reference internal" href="#compiling-and-linking">Compiling and linking</a><ul>
<li><a class="reference internal" href="#building-only-mdrun">Building only mdrun</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installing-gromacs">Installing GROMACS</a></li>
<li><a class="reference internal" href="#getting-access-to-gromacs-after-installation">Getting access to GROMACS after installation</a></li>
<li><a class="reference internal" href="#testing-gromacs-for-correctness">Testing GROMACS for correctness</a></li>
<li><a class="reference internal" href="#testing-gromacs-for-performance">Testing GROMACS for performance</a></li>
<li><a class="reference internal" href="#having-difficulty">Having difficulty?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#special-instructions-for-some-platforms">Special instructions for some platforms</a><ul>
<li><a class="reference internal" href="#building-on-windows">Building on Windows</a></li>
<li><a class="reference internal" href="#building-on-cray">Building on Cray</a></li>
<li><a class="reference internal" href="#building-on-solaris">Building on Solaris</a></li>
<li><a class="reference internal" href="#building-on-bluegene">Building on BlueGene</a><ul>
<li><a class="reference internal" href="#bluegene-q">BlueGene/Q</a></li>
<li><a class="reference internal" href="#fujitsu-primehpc">Fujitsu PRIMEHPC</a></li>
<li><a class="reference internal" href="#intel-xeon-phi">Intel Xeon Phi</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tested-platforms">Tested platforms</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../release-notes/older/index.html"
                        title="previous chapter">Release notes for older GROMACS versions</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../user-guide/index.html"
                        title="next chapter">User guide</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/install-guide/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../user-guide/index.html" title="User guide"
             >next</a> |</li>
        <li class="right" >
          <a href="../release-notes/older/index.html" title="Release notes for older GROMACS versions"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">GROMACS 2018.3</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, GROMACS development team.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.1.
    </div>
  </body>
</html>